# Requirements 

Blender 2.79 or any other 2.7 legacy version. The scripts will not work with more modern versions of Blender. However, another data set implementation known as [CLEVR-XAI](https://github.com/ahmedmagdiosman/clevr-xai) does not use legacy versions of Blender, which may be worth looking into to update the calls of this current replication.

The old implementation relied on Unix command calls. These were updated to be run on a Windows system.


# Instructions

Original 16 GB [data set](https://github.com/peter0749/object-referring/issues/2)


**Requires University of Malta email to access** the following [repo](https://drive.google.com/drive/folders/10fxiAk5ymHJR0VO2BNcAd2z3EevVdcUk?usp=sharing)

## Step 1:

Create the synthetic visual question answering dataset

- Path of the image generation script must be added to the site-packages of the python distribution
- From the image generation directory, call the windows blender installation and run the following command to generate 10 images

```
blender --background --python render_images.py -- --split train \
        --width 480 --height 320 --use_gpu 0 --start_idx 0 --num_images 10 
```
- Question generation from the question\_generation directory...

```
python generate_questions.py
```

## Step 2:

Repurpose the synthetic visual question answering dataset to create 

- Path of the image generation script must be added to the site-packages of the python distribution
- From the image generation directory, call the windows blender installation and run the following command with the option **clevr\_scene\_path** to pass the json of the original scenes

```
blender --background --python restore_render_images.py -- --split train \
        --width 480 --height 320 --use_gpu 0 --start_idx 0 --num_images 10 \
        --clevr_scene_path .\train_input\CLEVR_scenes.json
```

- Generate referring expressions using the json generated by the restore\_render\_images 

```
python generate_refexp.py \
    --template_dir clevr_ref+_templates/ \
    --input_scene_file ../output/clevr_ref+_scenes.json \
    --output_refexps_file ../output/clevr_ref+_train_refexps.json \
    --num_scenes 10 \
    --scene_start_idx 0
```

- Generate bounding boxes and segmentation masks


```
python get_box_mask_figure.py \
	--refexp_path ../output/clevr_ref+_train_refexps.json \
	--scene_path ../output/clevr_ref+_scenes.json \
	--img_dir_path ../output/images/ \
	--num_refexps 10 \
	--get_mask \
	--get_box 
```

## Step 3:

Preprocess and feed the data to the IEP-Ref model for training.

- Split your sets between validation and train, then extract features from train and validation. Unlike the original implementation, due to the low amount of images, batch size is 1. For an implementation with more data, bigger batch sizes make sense

```
python scripts/extract_features.py \
  --input_image_dir data/clevr_ref+_1.0/images/train/ \
  --output_h5_file data/train_features.h5 \
  --image_height 320 \
  --image_width 320 \
  --batch_size 1 

python scripts/extract_features.py \
  --input_image_dir data/clevr_ref+_1.0/images/val/ \
  --output_h5_file data/val_features.h5 \
  --image_height 320 \
  --image_width 320 \
  --batch_size 1 
```

After this, we preprocess the referring expressions to create a vocabulary file

```
python scripts/preprocess_refexps.py \
  --input_refexps_json data/clevr_ref+_1.0/refexps/clevr_ref+_train_refexps.json \
  --input_scenes_json data/clevr_ref+_1.0/scenes/clevr_ref+_train_scenes.json \
  --num_examples -1 \
  --output_h5_file data/train_refexps.h5 \
  --height 320 \
  --width 320 \
  --output_vocab_json data/vocab.json

python scripts/preprocess_refexps.py \
  --input_refexps_json data/clevr_ref+_1.0/refexps/clevr_ref+_val_refexps.json \
  --input_scenes_json data/clevr_ref+_1.0/scenes/clevr_ref+_val_scenes.json \
  --num_examples -1 \
  --output_h5_file data/val_refexps.h5 \
  --height 320 \
  --width 320 \
  --input_vocab_json data/vocab.json
```

Finally we train our model by training the generator, the execution engine, and both are trained using *REINFORCE*. The directories where the training happens must be manually created.

```
python scripts/train_model.py \
  --model_type PG \
  --num_iterations 30 \
  --num_train_samples 8 \
  --checkpoint_every 1 \
  --checkpoint_path data/run_PG_ref/program_generator.pt \
  --batch_size 1 \
  --train_refexp_h5 data/train_refexps.h5 \
  --train_features_h5 data/train_features.h5 \
  --val_refexp_h5 data/val_refexps.h5 \
  --val_features_h5 data/val_features.h5 \
  --vocab_json data/vocab.json \

python scripts/train_model.py \
  --program_generator_start_from ./data/run_PG_ref/program_generator.pt_1 \
  --train_execution_engine  1 \
  --train_program_generator 0 \
  --model_type PG+EE \
  --num_iterations 5 \
  --learning_rate 1e-4 \
  --checkpoint_path data/run_fixedPG+EE_ref/execution_engine.pt \
  --checkpoint_every 1 \
  --train_refexp_h5 data/train_refexps.h5 \
  --train_features_h5 data/train_features.h5 \
  --val_refexp_h5 data/val_refexps.h5 \
  --val_features_h5 data/val_features.h5 \
  --vocab_json data/vocab.json \
  --batch_size 1 \
  --feature_dim 1024,20,20

python scripts/train_model.py \
  --program_generator_start_from data/run_PG_ref/model/program_generator.pt_1 \
  --execution_engine_start_from data/run_fixedPG+EE_ref/model/execution_engine.pt_2 \
  --train_execution_engine  1 \
  --train_program_generator 1 \
  --model_type PG+EE \
  --num_iterations 20\
  --learning_rate 5e-5 \
  --checkpoint_path data/run_jointPG+EE_ref/joint_pg_ee.pt \
  --checkpoint_every 1 \
  --train_refexp_h5 data/train_refexps.h5 \
  --train_features_h5 data/train_features.h5 \
  --val_refexp_h5 data/val_refexps.h5 \
  --val_features_h5 data/val_features.h5 \
  --vocab_json data/vocab.json \
  --batch_size 1 \
  --feature_dim 1024,20,20

```

Testing the model can be performed as follows:

```
python scripts/run_model.py \
  --program_generator data/run\_PG\_ref/pg.pt \
  --execution_engine data/run\_gt\_EE/ee.pt \
  --input_refexp_h5 data/val_refexps.h5 \
  --input_features_h5 data/val_features.h5 \
  --batch_size 1 \
  --result_output_path ./data/result.json
```

The grounded truth option can be added to the last script to test the model using ground-truth programs as follows:

```
--use_gt_programs 1
```


# Bugs addressed

- CLEVR REf+ had mixed python 2 and 3 syntax, which created issues with debugging, imports, and string outputs (printing)
- Reduced number of samples generated for the sake replicability. 


